import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import re
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import google.generativeai as genai


# Cargar variables de entorno
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Configuraci√≥n del modelo Gemini
MODEL_CONFIG = {
    "temperature": 0.2,
    "top_p": 1,
    "top_k": 32,
    "max_output_tokens": 4096,
}

def get_gemini_model():
    """Inicializa y retorna el modelo Gemini Pro Vision para procesar PDFs"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        return model
    except Exception as e:
        st.error(f"Error al inicializar el modelo Gemini: {str(e)}")
        return None
def clean_extracted_text(text):
    """Limpia y normaliza el texto extra√≠do"""
    if not text:
        return ""
    
    # Eliminar caracteres no imprimibles
    text = ''.join(char for char in text if char.isprintable() or char in ['\n', '\t'])
    
    # Normalizar espacios en blanco
    text = re.sub(r'\s+', ' ', text)
    
    # Eliminar l√≠neas vac√≠as m√∫ltiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    # Asegurar que hay contenido sustancial
    if len(text.strip()) < 10:  # Ajusta este n√∫mero seg√∫n tus necesidades
        return ""
        
    return text.strip()
def process_pdf_with_gemini(pdf_file):
    """Procesa el PDF con Gemini Pro Vision usando el m√©todo de carga de archivos"""
    try:
        # Guardar temporalmente el archivo
        temp_path = f"temp_{pdf_file.name}"
        with open(temp_path, "wb") as f:
            f.write(pdf_file.getvalue())
        
        try:
            # Cargar el archivo usando el m√©todo recomendado por Google
            uploaded_file = genai.upload_file(temp_path)
            
            # Obtener el modelo
            model = get_gemini_model()
            if not model:
                return None
            
            # Instrucciones mejoradas para el procesamiento de PDFs escaneados
            prompt = """
            Analiza este documento PDF y extrae todo su contenido textual. Es muy importante que:

            1. Si el documento es un PDF escaneado o contiene im√°genes:
               - Realiza OCR detallado de TODO el texto visible
               - No omitas ning√∫n texto, por peque√±o que sea
               - Incluye n√∫meros, fechas y datos espec√≠ficos
               - Mant√©n el formato original (t√≠tulos, subt√≠tulos, p√°rrafos)

            2. Estructura el contenido de forma clara:
               - Respeta la jerarqu√≠a del documento
               - Preserva la separaci√≥n entre secciones
               - Mant√©n el orden original del texto

            3. Aseg√∫rate de incluir:
               - Texto en m√°rgenes o notas al pie
               - Encabezados y pies de p√°gina
               - Tablas y su contenido
               - Listas y enumeraciones

            Extrae absolutamente todo el contenido textual visible en el documento.
            """
            
            # Procesar con Gemini usando el archivo cargado
            response = model.generate_content(
                contents=[prompt, uploaded_file],
                generation_config=MODEL_CONFIG
            )
            
            # Limpiar y validar el texto extra√≠do
            extracted_text = clean_extracted_text(response.text)
            
            # Validar que se extrajo contenido √∫til
            if not extracted_text:
                st.warning(f"‚ö†Ô∏è El texto extra√≠do de {pdf_file.name} est√° vac√≠o despu√©s de la limpieza")
                return None
                
            # Mostrar informaci√≥n sobre el texto extra√≠do
            st.info(f"üìÑ Texto extra√≠do de {pdf_file.name}: {len(extracted_text)} caracteres")
            
            return extracted_text
            
        finally:
            # Limpiar el archivo temporal
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
    except Exception as e:
        st.error(f"Error al procesar el PDF: {str(e)}")
        return None


def get_pdf_text(pdf_docs):
    """Procesa m√∫ltiples PDFs"""
    text = ""
    for pdf in pdf_docs:
        try:
            st.info(f"Procesando {pdf.name} con Gemini Pro Vision...")
            
            # Verificar el tama√±o del archivo
            file_size = len(pdf.getvalue())
            size_mb = file_size / (1024 * 1024)
            st.info(f"Tama√±o del archivo: {size_mb:.2f} MB")
            
            # Procesar PDF con Gemini
            pdf_text = process_pdf_with_gemini(pdf)
            
            if pdf_text:
                text += f"\n--- Documento: {pdf.name} ---\n{pdf_text}\n"
                st.success(f"‚úÖ {pdf.name} procesado exitosamente")
            else:
                st.warning(f"‚ö†Ô∏è No se pudo extraer texto de {pdf.name}")
                
        except Exception as e:
            st.error(f"‚ùå Error procesando {pdf.name}: {str(e)}")
            continue
    
    if not text.strip():
        st.error("No se pudo extraer texto de ninguno de los PDFs.")
        return None
    
    return text

def get_text_chunks(text):
    """Divide el texto en fragmentos con configuraci√≥n optimizada"""
    if not text:
        return None
    
    try:
        # Configuraci√≥n m√°s granular para el divisor de texto
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # Chunks m√°s peque√±os para mejor procesamiento
            chunk_overlap=50,  # Menor superposici√≥n
            length_function=len,
            separators=["\n\n", "\n", " ", ""]  # Separadores m√°s espec√≠ficos
        )
        
        chunks = text_splitter.split_text(text)
        
        # Validar los chunks
        valid_chunks = [chunk for chunk in chunks if len(chunk.strip()) > 50]  # Filtrar chunks muy peque√±os
        
        if not valid_chunks:
            st.error("No se pudieron generar fragmentos de texto v√°lidos.")
            return None
        
        # Mostrar informaci√≥n sobre los chunks
        st.info(f"üìë Chunks generados: {len(valid_chunks)}")
        
        return valid_chunks
        
    except Exception as e:
        st.error(f"Error al dividir el texto: {str(e)}")
        return None

def get_vector_store(text_chunks):
    """Genera los embeddings y almacena en FAISS con validaci√≥n mejorada"""
    if not text_chunks:
        return None

    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        # Validar cada chunk antes de crear embeddings
        valid_chunks = []
        for i, chunk in enumerate(text_chunks):
            try:
                # Intentar crear un embedding de prueba
                _ = embeddings.embed_query(chunk)
                valid_chunks.append(chunk)
            except Exception as e:
                st.warning(f"Chunk {i} ignorado: no se pudo crear embedding")
                continue
        
        if not valid_chunks:
            st.error("No se pudieron crear embeddings v√°lidos.")
            return None
        
        # Crear el vector store con los chunks v√°lidos
        vector_store = FAISS.from_texts(valid_chunks, embedding=embeddings)
        
        # Crear directorio para el √≠ndice si no existe
        os.makedirs("faiss_index", exist_ok=True)
        
        # Guardar el √≠ndice y los datos
        vector_store.save_local("faiss_index")
        
        # Mostrar informaci√≥n sobre el vector store
        st.info(f"üíæ Vector store creado con {len(valid_chunks)} chunks")
        
        return vector_store
        
    except Exception as e:
        st.error(f"Error al crear el vector store: {str(e)}")
        return None

def get_qa_chain():
    """Configura el modelo de IA para responder preguntas"""
    prompt_template = """
    Responde la pregunta de la manera m√°s detallada posible usando el contexto proporcionado.
    Si la respuesta no est√° en el contexto, di "No encontr√© esa informaci√≥n en el documento".
    No inventes informaci√≥n que no est√© en el contexto.
    
    Contexto:
    {context}
    
    Pregunta:
    {question}
    
    Respuesta:
    """

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
    
    return PROMPT, model

def user_input(user_question):
    """Maneja las preguntas del usuario"""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        # Cargar el √≠ndice FAISS con la opci√≥n de deserializaci√≥n segura
        vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        
        # Obtener el prompt y el modelo
        prompt, llm = get_qa_chain()
        
        # Crear la cadena de QA
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(),
            chain_type_kwargs={"prompt": prompt}
        )
        
        # Obtener la respuesta
        response = qa_chain.invoke({"query": user_question})
        
        if response and "result" in response:
            st.write("Respuesta:", response["result"])
        else:
            st.error("No se pudo generar una respuesta.")

    except Exception as e:
        st.error(f"Error al procesar la pregunta: {str(e)}")

def main():
    """Aplicaci√≥n principal"""
    st.set_page_config(page_title="Chat PDF con Gemini 1.5")
    st.header("Chat con PDFs usando Gemini 1.5 Flash üìö")
    
    # Agregar advertencia de seguridad
    if not os.path.exists("faiss_index"):
        st.warning("""
        ‚ö†Ô∏è Nota de seguridad: Esta aplicaci√≥n utiliza almacenamiento local para los √≠ndices FAISS. 
        Solo procese documentos de fuentes confiables, ya que el sistema necesita deserializar 
        datos almacenados localmente.
        """)
    
    with st.sidebar:
        st.title("üìÅ Men√∫")
        pdf_docs = st.file_uploader(
            "Sube tus archivos PDF y haz clic en Procesar",
            accept_multiple_files=True,
            type=['pdf']
        )
        
        if st.button("üöÄ Procesar PDFs"):
            if not pdf_docs:
                st.error("Por favor, sube al menos un archivo PDF.")
                return
                
            with st.spinner("Procesando documentos..."):
                raw_text = get_pdf_text(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        vector_store = get_vector_store(text_chunks)
                        if vector_store:
                            st.success("¬°Procesamiento completado! üéâ")
                            st.info("Ahora puedes hacer preguntas sobre el contenido de los PDFs")

    # √Årea de chat
    st.subheader("üí≠ Pregunta sobre tus documentos")
    user_question = st.text_input("Escribe tu pregunta aqu√≠:")
    if user_question:
        user_input(user_question)

if __name__ == "__main__":
    main()